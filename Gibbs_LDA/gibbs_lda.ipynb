{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Volume 3: Gibbs Sampling and LDA\n",
    "    <Name>\n",
    "    <Class>\n",
    "    <Date>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import gammaln"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1\n",
    "\n",
    "Implement a Gibbs sampler for the exam scores problem (using the data in `examscores.npy`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gibbs(y, nu, tau2, alpha, beta, n_samples):\n",
    "    \"\"\"Gibbs sampler for the exam scores problem, assuming the\n",
    "    following likelihood and priors.\n",
    "        y_i    ~ N(mu, sigma2),\n",
    "        mu     ~ N(nu, tau2),\n",
    "        sigma2 ~ IG(alpha, beta),\n",
    "\n",
    "    Parameters:\n",
    "        y ((N,) ndarray): the exam scores.\n",
    "        nu (float): The prior mean parameter for mu.\n",
    "        tau2 (float > 0): The prior variance parameter for mu.\n",
    "        alpha (float > 0): The prior alpha parameter for sigma2.\n",
    "        beta (float < 0): The prior beta parameter for sigma2.\n",
    "        n_samples (int): the number of samples to draw.\n",
    "\n",
    "    Returns:\n",
    "        ((n_samples, 2) ndarray): The mu and sigma2 samples (as columns).\n",
    "    \"\"\"\n",
    "    raise NotImplementedError(\"Problem 1 Incomplete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2\n",
    "\n",
    "Plot the kernel density estimators for the posterior distributions of $\\mu$ and $\\sigma^2$.\n",
    "\n",
    "Next, use your samples of $\\mu$ and $\\sigma^2$ to draw samples from the posterior predictive distribution.\n",
    "Plot the kernel density estimator of your sampled scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3\n",
    "\n",
    "Implement `LDACGS.initialize()`.\n",
    "By randomly assigning initial topics, fill in the count matrices and topic assignment dictionary.\n",
    "In this method, you will initialize the count matrices (among other things).\n",
    "\n",
    "To be explicit, you will need to initialize $nmz$, $nzw$, and $nz$ to be zero arrays of the correct size.\n",
    "Then, in the second for loop, you will assign z to be a random integer in the correct range of topics.\n",
    "In the increment step, you need to figure out the correct indices to increment by one for each of the three arrays.\n",
    "Finally, assign $topics$ as given.\n",
    "\n",
    "## Problem 4\n",
    "\n",
    "Complete `LDACGS._sweep()`, which needs to iterate through each word of each document.\n",
    "It should call `LDACGS._conditional()` to get the conditional distribution at each iteration.\n",
    "\n",
    "Note that the first part of this method will undo what `LDACGS.initialize()` did.\n",
    "Then we will use the conditional distribution (instead of the uniform distribution we used previously) to pick a more accurate topic assignment.\n",
    "Finally, the latter part repeats what we did in `LDACGS.initialize()`, but does so using this more accurate topic assignment.\n",
    "\n",
    "## Problem 5\n",
    "\n",
    "Complete `LDACGS.sample()`.\n",
    "The argument `filename` is the name and location of a .txt file, where each line is considered a document.\n",
    "The corpus is built by `LDACGS.buildCorpus()`, and stopwords are removed (if argument `stopwords` is provided).\n",
    "Burn in the Gibbs sampler, computing and saving the log-likelihood with the method `_loglikelihood()`.\n",
    "After the burn in, iterate further, accumulating your count matrices, by adding `nzw` and `nmz` to `total_nzw` and `total_nmz` respectively, where you only add every `sample_rate`th iteration. \n",
    "Also save each log-likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LDACGS:\n",
    "    \"\"\"Do LDA with Gibbs Sampling.\"\"\"\n",
    "\n",
    "    def __init__(self, n_topics, alpha=0.1, beta=0.1):\n",
    "        \"\"\"Initialize system parameters.\"\"\"\n",
    "        self.n_topics = n_topics\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "\n",
    "    def buildCorpus(self, filename, stopwords_file=None):\n",
    "        \"\"\"Read the given filename and build the vocabulary.\"\"\"\n",
    "        with open(filename, 'r') as infile:\n",
    "            doclines = [line.rstrip().lower().split(' ') for line in infile]\n",
    "        n_docs = len(doclines)\n",
    "        self.vocab = list({v for doc in doclines for v in doc})\n",
    "        if stopwords_file:\n",
    "            with open(stopwords_file, 'r') as stopfile:\n",
    "                stops = stopfile.read().split()\n",
    "            self.vocab = [x for x in self.vocab if x not in stops]\n",
    "            self.vocab.sort()\n",
    "        self.documents = []\n",
    "        for i in range(n_docs):\n",
    "            self.documents.append({})\n",
    "            for j in range(len(doclines[i])):\n",
    "                if doclines[i][j] in self.vocab:\n",
    "                    self.documents[i][j] = self.vocab.index(doclines[i][j])\n",
    "\n",
    "    def initialize(self):\n",
    "        \"\"\"Initialize the three count matrices.\"\"\"\n",
    "        self.n_words = len(self.vocab)\n",
    "        self.n_docs = len(self.documents)\n",
    "        \n",
    "        # Initialize the three count matrices.\n",
    "        # The (i,j) entry of self.nmz is the number of words in document i assigned to topic j.\n",
    "        self.nmz = np.zeros((self.n_docs, self.n_topics))\n",
    "        # The (i,j) entry of self.nzw is the number of times term j is assigned to topic i.\n",
    "        self.nzw = np.zeros((self.n_topics, self.n_words))\n",
    "        # The (i)-th entry is the number of times topic i is assigned in the corpus.\n",
    "        self.nz = np.zeros(self.n_topics)\n",
    "\n",
    "        # Initialize the topic assignment dictionary.\n",
    "        self.topics = {} # key-value pairs of form (m,i):z\n",
    "\n",
    "        for m in range(self.n_docs):\n",
    "            for i in self.documents[m]:\n",
    "                # Get random topic assignment, i.e. z = ...\n",
    "                # Increment count matrices\n",
    "                # Store topic assignment, i.e. self.topics[(m,i)]=z\n",
    "                raise NotImplementedError(\"Problem 3 Incomplete\")\n",
    "\n",
    "    def sample(self,filename, burnin=100, sample_rate=10, n_samples=10, stopwords_file=None):\n",
    "        self.buildCorpus(filename, stopwords_file)\n",
    "        self.initialize()\n",
    "        self.total_nzw = np.zeros((self.n_topics, self.n_words))\n",
    "        self.total_nmz = np.zeros((self.n_docs, self.n_topics))\n",
    "        self.logprobs = np.zeros(burnin + sample_rate*n_samples)\n",
    "        for i in range(burnin):\n",
    "            # Sweep and store log likelihood.\n",
    "            raise NotImplementedError(\"Problem 5 Incomplete\")\n",
    "        for i in range(n_samples*sample_rate):\n",
    "            # Sweep and store log likelihood\n",
    "            raise NotImplementedError(\"Problem 5 Incomplete\")\n",
    "            if not i % sample_rate:\n",
    "                # accumulate counts\n",
    "                raise NotImplementedError(\"Problem 5 Incomplete\")\n",
    "\n",
    "    def phi(self):\n",
    "        phi = self.total_nzw + self.beta\n",
    "        self._phi = phi / np.sum(phi, axis=1)[:,np.newaxis]\n",
    "\n",
    "    def theta(self):\n",
    "        theta = self.total_nmz + self.alpha\n",
    "        self._theta = theta / np.sum(theta, axis=1)[:,np.newaxis]\n",
    "\n",
    "    def topterms(self,n_terms=10):\n",
    "        self.phi()\n",
    "        self.theta()\n",
    "        vec = np.atleast_2d(np.arange(0,self.n_words))\n",
    "        topics = []\n",
    "        for k in range(self.n_topics):\n",
    "            probs = np.atleast_2d(self._phi[k,:])\n",
    "            mat = np.append(probs,vec,0)\n",
    "            sind = np.array([mat[:,i] for i in np.argsort(mat[0])]).T\n",
    "            topics.append([self.vocab[int(sind[1,self.n_words - 1 - i])] for i in range(n_terms)])\n",
    "        return topics\n",
    "\n",
    "    def toplines(self,n_lines=5):\n",
    "        lines = np.zeros((self.n_topics,n_lines))\n",
    "        for i in range(self.n_topics):\n",
    "            args = np.argsort(self._theta[:,i]).tolist()\n",
    "            args.reverse()\n",
    "            lines[i,:] = np.array(args)[0:n_lines] + 1\n",
    "        return lines\n",
    "\n",
    "    def _removeStopwords(self, stopwords):\n",
    "        return [x for x in self.vocab if x not in stopwords]\n",
    "\n",
    "    def _conditional(self, m, w):\n",
    "        dist = (self.nmz[m,:] + self.alpha) * (self.nzw[:,w] + self.beta) / (self.nz + self.beta*self.n_words)\n",
    "        return dist / np.sum(dist)\n",
    "\n",
    "    def _sweep(self):\n",
    "        for m in range(self.n_docs):\n",
    "            for i in self.documents[m]:\n",
    "                # Retrieve vocab index for i-th word in document m.\n",
    "                # Retrieve topic assignment for i-th word in document m.\n",
    "                # Decrement count matrices.\n",
    "                # Get conditional distribution.\n",
    "                # Sample new topic assignment.\n",
    "                # Increment count matrices.\n",
    "                # Store new topic assignment.\n",
    "                raise NotImplementedError(\"Problem 4 Incomplete\")\n",
    "\n",
    "    def _loglikelihood(self):\n",
    "        lik = 0\n",
    "\n",
    "        for z in range(self.n_topics):\n",
    "            lik += np.sum(gammaln(self.nzw[z,:] + self.beta)) - gammaln(np.sum(self.nzw[z,:] + self.beta))\n",
    "            lik -= self.n_words * gammaln(self.beta) - gammaln(self.n_words*self.beta)\n",
    "\n",
    "        for m in range(self.n_docs):\n",
    "            lik += np.sum(gammaln(self.nmz[m,:] + self.alpha)) - gammaln(np.sum(self.nmz[m,:] + self.alpha))\n",
    "            lik -= self.n_topics * gammaln(self.alpha) - gammaln(self.n_topics*self.alpha)\n",
    "\n",
    "        return lik"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 6\n",
    "\n",
    "Create an `LDACGS` object with $20$ topics, letting $\\alpha$ and $\\beta$ be the default values.\n",
    "Run the Gibbs sampler, with a burn in of $100$ iterations, accumulating $10$ samples, only keeping the results of every $10$th sweep.\n",
    "Use `stopwords.txt` as the stopwords file.\n",
    "\n",
    "Plot the log-likelihoods. How long did it take to burn in?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 7\n",
    "\n",
    "Using `topterms()` to examine the topics for Reagan's addresses.\n",
    "Come up with labels for each topic.\n",
    "If `ntopics=20` and `n=10`, we will get the top $10$ words that represent each of the $20$ topics; for each topic, decide what these ten words jointly represent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
