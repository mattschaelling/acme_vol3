{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.linalg as la\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 12.11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Algorithm:\n",
    "\n",
    "$S_{t+1} = S_t + x_{t+1}x_{t+1}^T $\n",
    "\n",
    "$\\gamma_{t+1} = S_{t+1}^{-1}x_{t+1} $\n",
    "\n",
    "$e_{t+1} = y_{t+1} - x_{t+1}^T\\hat{\\beta}_t $\n",
    "\n",
    "$\\hat{\\beta}_{t+1} = \\hat{\\beta}_t + \\gamma_{t+1}e_{t+1} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, note that showing that $\\hat{\\beta}_{t+1} = S_{t+1}^{-1}X_{t+1}^TY_{t+1}$ is equivilant to showing $S_{t+1}\\hat{\\beta}_{t+1} = X_{t+1}^TY_{t+1}$. Now observe that $ X_{t+1}^TY_{t+1} = X_t^TY_t + x_{t+1}y_{t+1}$. Below we calculate using the proposed algorithm definitions above.\n",
    "\n",
    "$$\n",
    "S_{t+1}\\hat{\\beta}_{t+1} \n",
    "= S_{t+1}(\\hat{\\beta}_t + \\gamma_{t+1}e_{t+1}) \n",
    "= S_{t+1}(\\hat{\\beta}_t + (S_{t+1}^{-1}x_{t+1})(y_{t+1} - x_{t+1}^T\\hat{\\beta}_t)) \n",
    "= S_{t+1}\\hat{\\beta}_t + x_{t+1}(y_{t+1} - x_{t+1}^T\\hat{\\beta}_t) \n",
    "$$\n",
    "\n",
    "$$\n",
    "= (S_t + x_{t+1}x_{t+1}^T)(S_t^{-1}X_t^TY_t) + x_{t+1}(y_{t+1} - x_{t+1}^T\\hat{\\beta}_t) \n",
    "= X_t^TY_t + x_{t+1}x_{t+1}^TS_t^{-1}X_t^TY_t + x_{t+1}y_{t+1} - x_{t+1}x_{t+1}^T\\hat{\\beta}_t\n",
    "$$\n",
    "\n",
    "$$\n",
    "= X_t^TY_t + x_{t+1}y_{t+1} \n",
    "= X_{t+1}^TY_{t+1}\n",
    "$$\n",
    "\n",
    "And thus we see the algorithm works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 12.12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Sherman-Morrison-Woodbury formula is the following: \n",
    "$(A + uv^T)^{-1} = A^{-1} - \\frac{A^{-1}uv^TA^{-1}}{1 + v^TA^{-1}u}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now observe the following, using a straightforward application of the Sherman-Morrison-Woodbury:\n",
    "\n",
    "$$\n",
    "S_{t+1}^{-1} \n",
    "= (S_t + x_{t-1}x_{t+1}^T)^{-1} \n",
    "= S_t^{-1} - \\frac{S_t^{-1}x_{t+1}x_{t+1}^TS_t^{-1}}{1 + x_{t+1}^TS_t^{-1}x_{t+1}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 12.13\n",
    "\n",
    "Use the data from https://archive.ics.uci.edu/ml/datasets/BlogFeedback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = np.loadtxt('BlogFeedback/blogData_train.csv', \n",
    "                   delimiter=',')\n",
    "Xtrain = train[:,:-1]\n",
    "ytrain = train[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RLS(object):\n",
    "    \n",
    "    def __init__(self, X, Y, PCA=5):\n",
    "        '''\n",
    "        if PCA == -1 dont do PCA\n",
    "        ''' \n",
    "        n = X.shape[0]\n",
    "        if PCA != -1 and PCA < X.shape[1]:\n",
    "            self.PCA = True\n",
    "            self.Xmean = X.mean(axis=0)\n",
    "            X = X - self.Xmean\n",
    "            self.projection = la.svd(X, full_matrices=False)[-1][:PCA].T\n",
    "            X = X @ self.projection # reduce X down to PCA dimensions\n",
    "        else:\n",
    "            self.PCA = False\n",
    "        X = np.concatenate((np.ones(n).reshape((n,1)), X), axis=1)\n",
    "        #self.S = X.T @ X\n",
    "        self.S_inv = la.inv(X.T @ X)\n",
    "        self.beta = self.S_inv @ X.T @ Y\n",
    "    \n",
    "    def update(self,X,Y):\n",
    "        if self.PCA:\n",
    "            X = (X - self.Xmean) @ self.projection\n",
    "        X = np.concatenate(([1],X))\n",
    "        #self.S = self.S + np.outer(X,X)\n",
    "        Sx = self.S_inv @ X\n",
    "        self.S_inv = self.S_inv - (np.outer(Sx,Sx))/(1 + X @ Sx)\n",
    "        gamma = self.S_inv @ X\n",
    "        e = Y - X @ self.beta\n",
    "        self.beta = self.beta + gamma * e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "rls = RLS(Xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdata = np.loadtxt('BlogFeedback/blogData_test-2012.02.01.00_00.csv', \n",
    "                     delimiter=',')\n",
    "Xnew = newdata[0,:-1]\n",
    "ynew = newdata[0,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.clock()\n",
    "rls.update(Xnew, ynew)\n",
    "update_time = time.clock() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.clock()\n",
    "RLS(np.concatenate((Xtrain,Xnew.reshape(1,Xnew.shape[0]))), \n",
    "    np.concatenate((ytrain, [ynew])))\n",
    "ols_time = time.clock() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update Time:\t0.0002293335215044279\n",
      "OLS Time:\t1.9227170647934884\n"
     ]
    }
   ],
   "source": [
    "print(\"Update Time:\\t{}\\nOLS Time:\\t{}\".format(update_time, ols_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The complexity of the OLS solution is mega huge in comparison to the update, and it only gets bigger because n increases with each step. The update algorithm is primarily parameterized by the size of the feature space, which stays constant (here, we've chosen to project down to a 5-dimensional space plus a constant, so it's only 6 dimensional). However, the OLS fitting is parameterized by both the feature space size and the number of observations, so as time goes on the observations grow and the matrix multiplication costs increase, but for update the cost stays constant. As far as I can tell the order of the complexity is about the same for both (somewhere around square or cubic), especially if you use an alternate method instead of inverting $S_t$ in the OLS fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
