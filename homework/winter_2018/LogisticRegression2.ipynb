{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.api as sm\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the digits database and scikit-learn\n",
    "\n",
    "1. Split your data into 30% test and 70% training sets, \n",
    "2. For each of the values of $C = 10^{k}$ for $k = {-10,...,10}$ train an $L^{2}$ regularized logistic regression model with regularization weight lambda = $\\frac{1}{C}$  (this is the default form for scikit-learn) on the training set and compute the mean accuracy on the test set for each model.  Which performed best?  \n",
    "3. Repeat #2 with $L^{1}$ regularization instead of $L^{2}$.  Do the results suggest any features that can be dropped from the data set?\n",
    "4. Scikit-learn does not have logistic regression without regularization.  What values of $C$ are most similar to an un-regularized model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1) Split the data into .3 test and .7 train\n",
    "digits = datasets.load_digits()\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(digits['data'], \n",
    "                                                digits['target'], \n",
    "                                                test_size=.3, \n",
    "                                                train_size=.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best value of k = -2 with accuracy of 0.962962962963\n"
     ]
    }
   ],
   "source": [
    "# 2) Train models - L2 Norm\n",
    "accuracy = []\n",
    "\n",
    "# iterate through k\n",
    "for k in range(-10,11):\n",
    "    c = 10**k\n",
    "    # train and predict model for given c\n",
    "    model = LogisticRegression(penalty = 'l2', C=c)\n",
    "    model.fit(xtrain, ytrain)\n",
    "    ypredict = model.predict(xtest)\n",
    "    accuracy.append((k,np.mean(ypredict==ytest)))\n",
    "    \n",
    "# sort accuracies to find the best value of c\n",
    "accuracy = sorted(accuracy, key=lambda x: x[1], reverse=True)\n",
    "print(\"Best value of k = {} with accuracy of {}\".format(accuracy[0][0], accuracy[0][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best value of k = -1 with accuracy of 0.962962962963\n"
     ]
    }
   ],
   "source": [
    "# 3) Train models - L1 Norm\n",
    "accuracy = []\n",
    "\n",
    "# iterate through k\n",
    "for k in range(-10,11):\n",
    "    c = 10**k\n",
    "    # train and predict model for given c\n",
    "    model = LogisticRegression(penalty = 'l1', C=c)\n",
    "    model.fit(xtrain, ytrain)\n",
    "    ypredict = model.predict(xtest)\n",
    "    accuracy.append((k,np.mean(ypredict==ytest)))\n",
    "    \n",
    "# sort accuracies to find the best value of c\n",
    "accuracy = sorted(accuracy, key=lambda x: x[1], reverse=True)\n",
    "print(\"Best value of k = {} with accuracy of {}\".format(accuracy[0][0], accuracy[0][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients that are zero across all 10 classification models:\n",
      "[0, 1, 7, 8, 15, 16, 23, 24, 31, 32, 39, 40, 47, 48, 55, 56, 57]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAcMAAAEKCAYAAABuYT6iAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xd4c6a90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "c = 10**-1\n",
    "model = LogisticRegression(penalty = 'l1', C=c)\n",
    "model.fit(xtrain, ytrain)\n",
    "\n",
    "# to evaluate if any features are less-essential, \n",
    "# we will go through the 64 features and see which \n",
    "# are consistently 0 across all 10 classification models\n",
    "zeros = np.zeros_like(model.coef_[:,0])\n",
    "dropped_coefs = []\n",
    "for i in range(len(model.coef_[0])):\n",
    "    if np.all(model.coef_[:,i] == zeros):\n",
    "        dropped_coefs.append(i)\n",
    "        \n",
    "print(\"Coefficients that are zero across all 10 classification models:\\n{}\".format(dropped_coefs))\n",
    "dropped_coefs = np.array(dropped_coefs)\n",
    "plt.scatter(dropped_coefs%8 + 1, dropped_coefs//8 + 1)\n",
    "plt.title('Position of Pixels (Coefficients) Dropped from All 10 Logistic Regressions')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 4) Large values of C are most similar to unregularized model because the larger C is the smaller the regularization weight is (1/C) thus the less the optimizing process will be penalized/changed by the size of the norm of the coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify a classification problem related to your final project, using your project data.  \n",
    "\n",
    "1. Apply $L^2$ regularized logistic regression to model this with an appropriate choice of $C$ (or lambda).  Discuss how (and why) you chose your specific the value $C$.  \n",
    "2. Apply $L^1$ regularized logistic regression to model this with an appropriate choice of $C$.  Discuss how (and why) you chose your specific value of $C$.  \n",
    "3. Identify which features of your data to include and which to discard for a good logistic regression model for your problem.  Compare which features are suggested for removal by $L^1$ regularization (from `scikit-learn) versus using the methods we have used for linear regression, including p-values, BIC, and AIC (from statsmodels).  \n",
    "Clearly identify your final preferred model, and explain why you chose that over the other contenders. \n",
    "What conclusions can be drawn from your results about the original classification question you asked?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "To be candid, as of my current data there's not really any classification problem going on. The closest thing there is in the dataset is that I have an indicator for whether or not a day is on a weekend or not, and that was used as an explanatory variable for the models I estimated. Beyond that, I could fabricate classifications based on whether or not something is above or below certain thresholds, but I can't really think of any compelling reason why any of thsoe would give useful results. So, I'll just use the percent of UK purchases to predict whether or not it's a weekend and I'll use the total number of purchases to predict whether or not it's a weekend. Also, in this part of the dataset I only have 58 observations because they are daily aggregated observations, so I will be in danger of overfitting and sacrificing 10% of my data to validate is costly.\n",
    "\n",
    "To find the right value of C, I will do the same as above and iterate over possible values of $10^k$ for $k$ in ${-10, ... , 10}$ and choose the one that has the highest accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('purchases.csv')\n",
    "df.drop(df.columns[0], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>AUD_to_GBP</th>\n",
       "      <th>NEpurchases</th>\n",
       "      <th>UKpurchases</th>\n",
       "      <th>USpurchases</th>\n",
       "      <th>date</th>\n",
       "      <th>pctUKpurchases</th>\n",
       "      <th>weekend</th>\n",
       "      <th>AUD_to_GBP2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.507761</td>\n",
       "      <td>838.849246</td>\n",
       "      <td>697848.116857</td>\n",
       "      <td>1.874951e+06</td>\n",
       "      <td>2016-06-20</td>\n",
       "      <td>0.271152</td>\n",
       "      <td>0</td>\n",
       "      <td>0.257822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.508658</td>\n",
       "      <td>502.963546</td>\n",
       "      <td>689378.326053</td>\n",
       "      <td>1.504030e+06</td>\n",
       "      <td>2016-06-21</td>\n",
       "      <td>0.314223</td>\n",
       "      <td>0</td>\n",
       "      <td>0.258733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.510217</td>\n",
       "      <td>856.491542</td>\n",
       "      <td>535229.615066</td>\n",
       "      <td>3.673902e+06</td>\n",
       "      <td>2016-06-22</td>\n",
       "      <td>0.127133</td>\n",
       "      <td>0</td>\n",
       "      <td>0.260321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.513378</td>\n",
       "      <td>248.206220</td>\n",
       "      <td>457024.954462</td>\n",
       "      <td>1.698697e+06</td>\n",
       "      <td>2016-06-23</td>\n",
       "      <td>0.211981</td>\n",
       "      <td>0</td>\n",
       "      <td>0.263557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.546301</td>\n",
       "      <td>390.193942</td>\n",
       "      <td>37645.488798</td>\n",
       "      <td>1.053211e+06</td>\n",
       "      <td>2016-06-24</td>\n",
       "      <td>0.034498</td>\n",
       "      <td>0</td>\n",
       "      <td>0.298445</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  AUD_to_GBP  NEpurchases    UKpurchases   USpurchases  \\\n",
       "0           0    0.507761   838.849246  697848.116857  1.874951e+06   \n",
       "1           1    0.508658   502.963546  689378.326053  1.504030e+06   \n",
       "2           2    0.510217   856.491542  535229.615066  3.673902e+06   \n",
       "3           3    0.513378   248.206220  457024.954462  1.698697e+06   \n",
       "4           4    0.546301   390.193942   37645.488798  1.053211e+06   \n",
       "\n",
       "         date  pctUKpurchases  weekend  AUD_to_GBP2  \n",
       "0  2016-06-20        0.271152        0     0.257822  \n",
       "1  2016-06-21        0.314223        0     0.258733  \n",
       "2  2016-06-22        0.127133        0     0.260321  \n",
       "3  2016-06-23        0.211981        0     0.263557  \n",
       "4  2016-06-24        0.034498        0     0.298445  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xtrain, xtest, ytrain, ytest = train_test_split(df[['NEpurchases', 'UKpurchases', 'USpurchases', 'pctUKpurchases']], \n",
    "                                                df['weekend'], test_size = .2, train_size=.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best value of k = 1 with accuracy of 0.916666666667\n"
     ]
    }
   ],
   "source": [
    "# First we consider a model with the pct of goods purchases from the UK\n",
    "accuracy = []\n",
    "for k in range(-10,11):   \n",
    "    c = 10**k\n",
    "    # train and predict model for given c\n",
    "    model = LogisticRegression(penalty = 'l2', C=c)\n",
    "    model.fit(xtrain['pctUKpurchases'].values.reshape(-1,1), ytrain)\n",
    "    ypredict = model.predict(xtest['pctUKpurchases'].values.reshape(-1,1))\n",
    "    accuracy.append((k,np.mean(ypredict==ytest)))\n",
    "\n",
    "accuracy = sorted(accuracy, key=lambda x: x[1], reverse=True)\n",
    "print(\"Best value of k = {} with accuracy of {}\".format(accuracy[0][0], accuracy[0][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best value of k = -10 with accuracy of 0.916666666667\n"
     ]
    }
   ],
   "source": [
    "# Next we consider a model with total Australian consumer expenditures on companies from\n",
    "# the Netherlands, UK, and US\n",
    "accuracy = []\n",
    "for k in range(-10,11):   \n",
    "    c = 10**k\n",
    "    # train and predict model for given c\n",
    "    model = LogisticRegression(penalty = 'l2', C=c)\n",
    "    model.fit(xtrain[['NEpurchases', 'UKpurchases', 'USpurchases']], ytrain)\n",
    "    ypredict = model.predict(xtest[['NEpurchases', 'UKpurchases', 'USpurchases']])\n",
    "    accuracy.append((k,np.mean(ypredict==ytest)))\n",
    "\n",
    "accuracy = sorted(accuracy, key=lambda x: x[1], reverse=True)\n",
    "print(\"Best value of k = {} with accuracy of {}\".format(accuracy[0][0], accuracy[0][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best value of k = 0 with accuracy of 1.0\n"
     ]
    }
   ],
   "source": [
    "# Here, we only consider the second model since we want to see if it \n",
    "# is going to drop any of the features\n",
    "accuracy = []\n",
    "for k in range(-10,11):   \n",
    "    c = 10**k\n",
    "    # train and predict model for given c\n",
    "    model = LogisticRegression(penalty = 'l1', C=c)\n",
    "    model.fit(xtrain[['NEpurchases', 'UKpurchases', 'USpurchases']], ytrain)\n",
    "    ypredict = model.predict(xtest[['NEpurchases', 'UKpurchases', 'USpurchases']])\n",
    "    accuracy.append((k,np.mean(ypredict==ytest)))\n",
    "\n",
    "accuracy = sorted(accuracy, key=lambda x: x[1], reverse=True)\n",
    "print(\"Best value of k = {} with accuracy of {}\".format(accuracy[0][0], accuracy[0][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.02992958e-03,   7.97248066e-06,  -1.24400165e-05]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Next, we look at the features to see if they should be dropped\n",
    "c = 1\n",
    "# train and predict model for given c\n",
    "model = LogisticRegression(penalty = 'l1', C=c)\n",
    "model.fit(xtrain[['NEpurchases', 'UKpurchases', 'USpurchases']], ytrain)\n",
    "ypredict = model.predict(xtest[['NEpurchases', 'UKpurchases', 'USpurchases']])\n",
    "accuracy.append((k,np.mean(ypredict==ytest)))\n",
    "model.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see here that UK Purchases and US purchases are the smallest in magnitued, suggesting perhaps that they are the least significant. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.680794\n",
      "         Iterations 4\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                weekend   No. Observations:                   58\n",
      "Model:                          Logit   Df Residuals:                       57\n",
      "Method:                           MLE   Df Model:                            0\n",
      "Date:                Fri, 19 Jan 2018   Pseudo R-squ.:                 -0.1558\n",
      "Time:                        01:44:04   Log-Likelihood:                -39.486\n",
      "converged:                       True   LL-Null:                       -34.162\n",
      "                                        LLR p-value:                       nan\n",
      "==================================================================================\n",
      "                     coef    std err          z      P>|z|      [95.0% Conf. Int.]\n",
      "----------------------------------------------------------------------------------\n",
      "pctUKpurchases    -1.0965      0.928     -1.181      0.238        -2.916     0.723\n",
      "==================================================================================\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.248301\n",
      "         Iterations 8\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                weekend   No. Observations:                   58\n",
      "Model:                          Logit   Df Residuals:                       55\n",
      "Method:                           MLE   Df Model:                            2\n",
      "Date:                Fri, 19 Jan 2018   Pseudo R-squ.:                  0.5784\n",
      "Time:                        01:44:04   Log-Likelihood:                -14.401\n",
      "converged:                       True   LL-Null:                       -34.162\n",
      "                                        LLR p-value:                 2.618e-09\n",
      "===============================================================================\n",
      "                  coef    std err          z      P>|z|      [95.0% Conf. Int.]\n",
      "-------------------------------------------------------------------------------\n",
      "NEpurchases     0.0030      0.001      2.108      0.035         0.000     0.006\n",
      "UKpurchases  8.305e-06   3.04e-06      2.732      0.006      2.35e-06  1.43e-05\n",
      "USpurchases -8.793e-06   2.64e-06     -3.337      0.001      -1.4e-05 -3.63e-06\n",
      "===============================================================================\n"
     ]
    }
   ],
   "source": [
    "# estimate model1\n",
    "model1 = sm.Logit(df['weekend'], df[['pctUKpurchases']])\n",
    "results1 = model1.fit()\n",
    "print(results1.summary())\n",
    "\n",
    "# estimate model1\n",
    "model2 = sm.Logit(df['weekend'], df[['NEpurchases', 'UKpurchases', 'USpurchases']])\n",
    "results2 = model2.fit()\n",
    "print(results2.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's some interesting results, we seet hat NE Purchases are the least significant of the variables in the second model, where I would have guessed it was the most significant from the Scikit Learn regressions coefficient results, since it's coefficient is the largest. Perhaps there is ust more variance in the data resulting in more ambiguity regarding it's true effect. Because of that, if this was something I thought was important to predict (which I don't think it is), I would choose the model with UK Purchases and US Purchases since they are both significant. My gut feeling is to trust these results backed-up with statistical theory than the results from algorithms because I am less familiar with them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
