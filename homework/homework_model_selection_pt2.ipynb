{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import linear_model\n",
    "from scipy.misc import comb as comb1\n",
    "from scipy.special import gamma\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import scipy.linalg as la"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 10.17"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Return to the problem about the city in which there have been 496 female\n",
    "births and 489 male births in the past year. We assume the number of female\n",
    "births is binomially distributed with parameter $\\theta$, and we are interested in\n",
    "comparing two possible models: $M_0$ is the model where $\\theta$ = 1/2 and $M_1$, is\n",
    "a model where $\\theta$ can take any value in [0, 1]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(i) Compute $p(D | M)$ for each model, assuming a prior for $\\theta$ under $M_1$ of\n",
    "$p(\\theta | M_1) = B(1, 1)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nfemale = 496\n",
    "nmale = 489\n",
    "# p(D|M_i)\n",
    "dataprob_model0 = comb1(nfemale+nmale, nfemale)*(.5)**(nfemale+nmale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$p(D|M_1) \n",
    "= \\frac{p(D|\\Theta, M_1) P(\\Theta|M_1)}{P(\\Theta|D,M_1)} \n",
    "= \\frac{{985 \\choose 496}\\Theta^{496} (1-\\Theta)^{489} \\Theta^0(1-\\Theta)^0 \\frac{\\Gamma(2)}{\\Gamma(1)\\Gamma(1)}}{\\Theta^{1+496-1}(1-\\Theta)^{1+489-1} \\frac{\\Gamma(2+985)}{\\Gamma(1+496)\\Gamma(1+489)}} $\n",
    "\n",
    "Thus, we see that \n",
    "$ p(D|M_1) \n",
    "= {985 \\choose 496} \\frac{\\Gamma(497)\\Gamma(490)}{\\Gamma(987)}\n",
    "= \\frac{985! 496! 489!}{496! 489! 986!} \n",
    "= \\frac{1}{986} $\n",
    "because all of the terms with $\\Theta$ cancel out and $\\frac{\\Gamma(2)}{\\Gamma(1)\\Gamma(1)} = 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#logdataprob_model1 = np.log(comb(nfemale+nmale, nfemale)) + np.log(gamma(497)) + np.log(gamma(490)) - np.log(gamma(987))\n",
    "dataprob_model1 = 1/986."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(D|M_0) = 0.0247925009971\n",
      "P(D|M_1) = 0.00101419878296\n"
     ]
    }
   ],
   "source": [
    "print('P(D|M_0) = {}\\nP(D|M_1) = {}'.format(dataprob_model0, dataprob_model1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(ii) Assuming a prior of $p(M_i)=1/2$ for both models, compute $p(M | D)$\n",
    "for each model. By this measure which model is preferred?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "$ P(M_i|D) \n",
    "= \\frac{P(D|M_i) .5}{P(D|M_0).5 + P(D|M_1).5} \n",
    "= \\frac{P(D|M_i)}{P(D|M_0) + P(D|M_1)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(M_0|D) = 0.960700175086\n",
      "P(M_1|D) = 0.0392998249139\n"
     ]
    }
   ],
   "source": [
    "total_dataprob = dataprob_model0 + dataprob_model1\n",
    "model0prob = dataprob_model0 / total_dataprob\n",
    "model1prob = dataprob_model1 / total_dataprob\n",
    "print(\"P(M_0|D) = {}\\nP(M_1|D) = {}\".format(model0prob, model1prob))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 10.18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit a linear model to the wages.csv dataset using scikit-learn instead of\n",
    "statsmodels. To do this use the commands:\n",
    "\n",
    "from sklearn import linear_model\n",
    "\n",
    "regr = linear_model.LinearRegression()\n",
    "\n",
    "X = df[['female','educ','exper','tenure','married','female*married','numdep','nonwhite']]\n",
    "\n",
    "y = df['wage']\n",
    "\n",
    "regr.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the calculated coefficients regr.coef_ with the coefficients computed\n",
    "with statsmodels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.34413527,  0.56308905,  0.02091295,  0.12976128,  1.73557801,\n",
       "       -2.3578156 ,  0.08909389, -0.2142343 ])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('wages.csv')\n",
    "df['female*married'] = df['female']*df['married']\n",
    "regr = linear_model.LinearRegression()\n",
    "X = df[['female','educ','exper','tenure','married','female*married','numdep','nonwhite']]\n",
    "y = df['wage']\n",
    "regr.fit(X,y)\n",
    "regr.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                   wage   R-squared:                       0.393\n",
      "Model:                            OLS   Adj. R-squared:                  0.384\n",
      "Method:                 Least Squares   F-statistic:                     41.90\n",
      "Date:                Fri, 08 Dec 2017   Prob (F-statistic):           1.42e-51\n",
      "Time:                        17:53:48   Log-Likelihood:                -1301.6\n",
      "No. Observations:                 526   AIC:                             2621.\n",
      "Df Residuals:                     517   BIC:                             2660.\n",
      "Df Model:                           8                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==================================================================================\n",
      "                     coef    std err          t      P>|t|      [95.0% Conf. Int.]\n",
      "----------------------------------------------------------------------------------\n",
      "const             -2.5664      0.780     -3.289      0.001        -4.099    -1.033\n",
      "female            -0.3441      0.414     -0.832      0.406        -1.157     0.469\n",
      "educ               0.5631      0.051     10.991      0.000         0.462     0.664\n",
      "exper              0.0209      0.012      1.735      0.083        -0.003     0.045\n",
      "tenure             0.1298      0.021      6.219      0.000         0.089     0.171\n",
      "married            1.7356      0.409      4.240      0.000         0.931     2.540\n",
      "female*married    -2.3578      0.533     -4.421      0.000        -3.406    -1.310\n",
      "numdep             0.0891      0.108      0.827      0.409        -0.123     0.301\n",
      "nonwhite          -0.2142      0.421     -0.509      0.611        -1.041     0.612\n",
      "==============================================================================\n",
      "Omnibus:                      183.094   Durbin-Watson:                   1.771\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              679.683\n",
      "Skew:                           1.578   Prob(JB):                    2.56e-148\n",
      "Kurtosis:                       7.588   Cond. No.                         163.\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "X = sm.add_constant(X)\n",
    "print(sm.OLS(y,X).fit().summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus the coefficients are the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 10.19"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using sklearnâ€™s test-train tools create a training set that is 70% of the full\n",
    "data and a test set that is 30% of the full data. Fit both a full model (all\n",
    "the features listed above) and a model having only the features ['educ','\n",
    "tenure','married','female*married'] on the training data and compare\n",
    "the Mean Square Error (MSE = score in scikit-learn) for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE for Full Model:\t\t8.01842306049\n",
      "MSE for Restricted Model:\t7.98223145876\n"
     ]
    }
   ],
   "source": [
    "train, test = train_test_split(df, test_size=.3, train_size = .7)\n",
    "\n",
    "regr_full_model = linear_model.LinearRegression()\n",
    "X1 = train[['female','educ','exper','tenure','married','female*married','numdep','nonwhite']]\n",
    "y1 = train['wage']\n",
    "regr_full_model.fit(X1,y1)\n",
    "\n",
    "testX1 = test[['female','educ','exper','tenure','married','female*married','numdep','nonwhite']]\n",
    "testy = test['wage']\n",
    "predict1 = np.dot(testX1, regr_full_model.coef_) + regr_full_model.intercept_\n",
    "mse1 = mean_squared_error(testy, predict1)\n",
    "\n",
    "\n",
    "regr_restricted_model = linear_model.LinearRegression()\n",
    "X2 = train[['educ','tenure','married','female*married']]\n",
    "regr_restricted_model.fit(X2,y1)\n",
    "\n",
    "testX2 = test[['educ','tenure','married','female*married']]\n",
    "predict2 = np.dot(testX2, regr_restricted_model.coef_) + regr_restricted_model.intercept_\n",
    "mse2 = mean_squared_error(testy, predict2)\n",
    "\n",
    "print(\"MSE for Full Model:\\t\\t{}\\nMSE for Restricted Model:\\t{}\".format(mse1,mse2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 10.20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use scikit-learn to compare each of the two previous models with a 7-fold\n",
    "cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "regr_full_model = linear_model.LinearRegression()\n",
    "X = df[['female','educ','exper','tenure','married','female*married','numdep','nonwhite']]\n",
    "y = df['wage']\n",
    "cv_score1 = -1*cross_val_score(regr_full_model, X,y, scoring='neg_mean_squared_error', cv=7)\n",
    "\n",
    "regr_restricted_model = linear_model.LinearRegression()\n",
    "X_restricted = df[['educ','tenure','married','female*married']]\n",
    "cv_score2 = -1*cross_val_score(regr_restricted_model, X_restricted, y, scoring='neg_mean_squared_error', cv=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Model MSE:\t\t[ 15.04807859   7.80036641   8.23095008   9.70621285   4.45192344\n",
      "   7.8667286    8.0232463 ]\n",
      "Restricted Model MSE:\t[ 14.95477276   7.37446954   8.30851754   9.84279749   4.61098581\n",
      "   7.41039223   7.89874299]\n",
      "\n",
      "\n",
      "Full Model MSE Normed:\t\t24.3993240648\n",
      "Restricted Model MSE Normed:\t24.1351486303\n"
     ]
    }
   ],
   "source": [
    "print(\"Full Model MSE:\\t\\t{}\\nRestricted Model MSE:\\t{}\".format(cv_score1, cv_score2))\n",
    "print(\"\\n\\nFull Model MSE Normed:\\t\\t{}\\nRestricted Model MSE Normed:\\t{}\".format(la.norm(cv_score1), la.norm(cv_score2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we take the 2-norm of the vector of mean squared errors to compare the two models. We see that the restricted model does a bit better, which makes sense because it would have less of a problem with overfitting the training data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
