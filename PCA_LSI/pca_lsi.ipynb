{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import string\n",
    "import numpy as np\n",
    "from math import log\n",
    "from scipy import sparse\n",
    "from sklearn import datasets\n",
    "from scipy import linalg as la\n",
    "from collections import Counter\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.sparse import linalg as spla\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.dpi\"] = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1\n",
    "\n",
    "Recreate Figure 18.4 by performing PCA on the iris dataset, keeping the first two principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "Y = X - X.mean(axis=0)\n",
    "U,S,VT = la.svd(Y, full_matrices=False)\n",
    "S**2/(S**2).sum() # variance percentages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2\n",
    "\n",
    "Using the techniques of LSI, applied to the word count matrix $X$, and keeping the first 7 principal components, find the most similar and least similar speeches to both Bill Clinton's 1993 speech and to Ronald Reagan's 1984 speech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of filepaths to each text file in the folder.\n",
    "folder = \"./Addresses/\"\n",
    "paths = [folder+p for p in os.listdir(folder) if p[-4:]==\".txt\"]\n",
    "\n",
    "# Helper function to get list of words in a string.\n",
    "def extractWords(text):\n",
    "    ignore = string.punctuation + string.digits\n",
    "    cleaned = \"\".join([t for t in text.strip() if t not in ignore])\n",
    "    return cleaned.lower().split()\n",
    "\n",
    "# Initialize vocab set, then read each file and add to the vocab set.\n",
    "vocab = set()\n",
    "for p in paths:\n",
    "    with open(p, 'r') as infile:\n",
    "        for line in infile:\n",
    "            vocab.update(extractWords(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load stopwords\n",
    "with open(\"stopwords.txt\", 'r') as f:\n",
    "    stops = set([w.strip().lower() for w in f.readlines()])\n",
    "\n",
    "# remove stopwords from vocabulary, create ordering\n",
    "vocab = {w:i for i, w in enumerate(vocab.difference(stops))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = []      # holds the entries of X\n",
    "doc_index = []   # holds the row index of X\n",
    "word_index = []  # holds the column index of X\n",
    "\n",
    "# Iterate through the documents.\n",
    "for doc, p in enumerate(paths):\n",
    "    with open(p, 'r') as f:\n",
    "        # Create the word counter.\n",
    "        ctr = Counter()\n",
    "        for line in f:\n",
    "            ctr.update(extractWords(line))\n",
    "        # Iterate through the word counter, store counts.\n",
    "        for word, count in ctr.items():\n",
    "            if word in vocab:\n",
    "                word_index.append(vocab[word])\n",
    "                counts.append(count)\n",
    "                doc_index.append(doc)\n",
    "\n",
    "# Create sparse matrix holding these word counts.\n",
    "X = sparse.csr_matrix((counts, [doc_index, word_index]),\n",
    "                       shape=(len(paths), len(vocab)), dtype=np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3\n",
    "\n",
    "Repeat Problem 2 using the matrix $A$.\n",
    "Do your answers seem more reasonable than before?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.zeros(len(vocab))\n",
    "counts = []\n",
    "doc_index = []\n",
    "word_index = []\n",
    "\n",
    "# get doc-term counts and global term counts\n",
    "for doc, path in enumerate(paths):\n",
    "    with open(path, 'r') as f:\n",
    "        # create the word counter\n",
    "        ctr = Counter()\n",
    "        for line in f:\n",
    "            words = extractWords(line)\n",
    "            ctr.update(words)\n",
    "        # iterate through the word counter, store counts\n",
    "        for word, count in ctr.items():\n",
    "            if word in vocab:\n",
    "                word_ind = vocab[word]\n",
    "                word_index.append(word_ind)\n",
    "                counts.append(count)\n",
    "                doc_index.append(doc)\n",
    "                t[word_ind] += count\n",
    "\n",
    "# Get global weights.\n",
    "g = np.ones(len(vocab))\n",
    "logM = log(len(paths))\n",
    "for count, word in zip(counts, word_index):\n",
    "    p = count/float(t[word])\n",
    "    g[word] += p*log(p+1)/logM\n",
    "\n",
    "# Get globally weighted counts.\n",
    "gwcounts = []\n",
    "for count, word in zip(counts, word_index):\n",
    "    gwcounts.append(g[word]*log(count+1))\n",
    "\n",
    "# Create sparse matrix holding these globally weighted word counts\n",
    "A = sparse.csr_matrix((gwcounts, [doc_index,word_index]),\n",
    "                      shape=(len(paths), len(vocab)), dtype=np.float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
